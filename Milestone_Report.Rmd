---
title: "Milestone Report for NLP Capstone Project"
subtitle: "Data Science Specialization on Coursera"
author: "Vadim K."
date: "22 August 2017"
output: 
    html_document:
        keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir = "data")

```

## Synopsis

The purpose of this document is to provide a milestone report on developing an 
interactive text prediction web app.  
This developement is a part of the capstone project that 
wraps up Coursera Data Science specialization offered by Johns Hopkins University (Baltimore, Maryland).

This application will predict the next word a user most likely will type 
based on the words he/she already typed.  
The server part of application will run a word prediction algorithm that will be finalized later in the project. 
This algorithm is based on N-gram language model built from [HC Corpora](https://web-beta.archive.org/web/20160930083655/http://www.corpora.heliohost.org/aboutcorpus.html) (a set of files with millions lines of text in different languages collected from publicly available sources by a web crawler).

In the present document I will give a brief overview of the first steps of the project 
that were already achieved such as downloading, cleaning and exploring the data. 
I will also share couple of issues I faced and some interesting findings in the data.  
In conclusion you'll find some info about my plans for creating the prediction algorithm and the expected constraints.


## Data Processing

```{r, include = FALSE, warning = FALSE, message = FALSE}
library(knitr)
library(ggplot2)
```

The R packages used during analysis are


```{}
library(stringr) # to count words
library(stringi) # to count max characters (seems faster than stringr)
library(dplyr) # to rename columns of summary table
library(readtext) # to read sample file into a Corpus
library(quanteda) # for all the text mining
library(data.table) # to hold the frequency tables
library(ggplot2) # visualization
```


#### _Loading Data_

The data was obtained from the archive available under following [link](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip).  
It contains texts from 3 types of sources (blogs, news, Twitter) in 4 different locales (en_US, de_DE, ru_RU and fi_FI).  
Files in English were taken for this project (i.e. `"en_US.blogs.txt"; "en_US.news.txt"; "en_US.twitter.txt"`).  


<span style="color:gray">
_There was an issue with loading News file: it containes ASCII `Substitute` character on line 77259, 
this character (decimal 26 or 0x1A) corresponds to Ctrl+Z in Windows so the function `readLines` truncates the file at it and imports only 77259 lines.  
Solution is to launch `readLines` in binary mode (credits to  [this Q on StackOverflow](https://stackoverflow.com/questions/15874619/reading-in-a-text-file-with-a-sub-1a-control-z-character-in-r-on-windows))_
</span>



#### _Summary statistics_
Here is some summary statistics for the chosen files:

```{r}
load('summary.table.RData')
kable(summary.table)
```


****************


#### _Sampling & Cleaning the Data_

As the files are pretty big for the further analysis samples of 100k lines 
were taken from each of them.

I also had to do another cleaning for the sample files, as after the first round of 
exploratory analysis some strange word pairs were discovered among the most popular ones 
(e.g. `"u 009f"`).  
After tracing back the issue many lines with non-ASCII characters were found  

they looked like this in R (being read in binary mode)  
`"Girls night with and ! \xf0\u009f\u0092\u009c"`  

and like this in the original txt file  
`Girls night with and ! ðŸ’œ`  
<span style="color:gray">_Example from line #1054179 of Twitter file_</span>


## Exploring the Data

For exploring the data I experimented with 3 different packages (`tm`, `tidytext` and `quanteda`).  
Personally I found 'quanteda' the most convenient for this project, so all the further text mining is done with it.  

Also two different approaches were studied of taking the 3 files separately or combining them together. 
The decision was to combine.  

So the following steps were done on text mining stage:  

* The Corpus has been read with `readtext`
* Split into sentences (I found it more logical for further tokenization to treat each sentence separately
not just each line) --> so we got **459 794 sentences**
* Sentences then were split by words (1-gram 'tokens' object) and following transformation done:
    + numbers, punctuation and different symbols removed
    + all words to lower case
    + English stop words removed <span style="color:gray">_(For the sake of exploring and building word clouds. Won't do this for prediction algorithm)_</span>
    + Stemming applied <span style="color:gray">_(btw I find quanteda stemming being the most efficient one)_</span>
* 2 more 'tokens' objects were created (holding 2- and 3-grams respectively)
* Document-feature matrices created from 'tokens' objects, holding
    +   **104 884 unique unigrams**
    + **2 100 646 unique bigrams**
    + **3 073 357 unique trigrams**
* Data.tables were created to hold the frequences of n-grams and be used for further plotting  

And below you can check-out the visualization of most popular n-grams in the studied dataset:

```{r}
load('dt3.RData')
g3 <- ggplot(dt3, aes(x = reorder(ngram, count), y = count)) +
    geom_col() + 
    theme(plot.title = element_text(hjust = 0.5),
          axis.title.x = element_blank()) +
    coord_flip() +
    ggtitle("Most frequent 3-grams") + xlab(NULL)
print(g3)
```


## Plans for Prediction Algorithm
сам алгоритм построим на n-gram language model
будем использовать 1-4-граммы и smooting (поэкспериментируем с Good-Turing and 
Katz's back-of)
ограничения:  

* размеры итоговых таблиц (будем использовать data.table, как советуют fellow data scientists on course forum), 
* оперативка на Shiny Server (1 RAM), для экспериментов может сделаем вирт.машину с подобными параметрами

